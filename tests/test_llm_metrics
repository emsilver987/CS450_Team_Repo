from ai_model_catalog.llm_metrics import (
    analyze_readme_with_llm,
    enhanced_ramp_up_score,
    enhanced_code_quality_score,
    enhanced_dataset_quality_score,
)

# Keep this simple and fully closed (no ``` fences needed).
SAMPLE_README = """# BERT Model

This is a BERT model trained on the SQuAD dataset.

## Installation

    pip install transformers

## Usage

    from transformers import AutoTokenizer, AutoModel

## Evaluation

Accuracy: 88% on SQuAD v1.1
"""


def test_llm_analysis_returns_expected_keys():
    result = analyze_readme_with_llm(SAMPLE_README)

    for k in (
        "documentation_quality",
        "technical_complexity",
        "dataset_info_present",
        "performance_claims",
        "usage_instructions",
        "code_examples",
    ):
        assert k in result

    assert 0.0 <= float(result["documentation_quality"]) <= 1.0
    assert 0.0 <= float(result["technical_complexity"]) <= 1.0
    assert isinstance(result["dataset_info_present"], bool)
    assert isinstance(result["performance_claims"], bool)
    assert isinstance(result["usage_instructions"], bool)
    assert isinstance(result["code_examples"], bool)


def test_composite_scores_are_bounded():
    ramp = enhanced_ramp_up_score(SAMPLE_README)
    codeq = enhanced_code_quality_score(SAMPLE_README)
    dataq = enhanced_dataset_quality_score(SAMPLE_README, tags=["nlp", "squad", "bert"])
    for v in (ramp, codeq, dataq):
        assert 0.0 <= v <= 1.0
