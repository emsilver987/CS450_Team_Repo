#!/usr/bin/env bash

set -euo pipefail

# -------------------
# Logging helpers
# -------------------
log_info() {
    if [[ "${LOG_VERBOSE:-false}" == "true" && -n "${LOG_FILE:-}" ]]; then
        echo "[INFO] $1" >> "$LOG_FILE"
    fi
}
log_warn() {
    if [[ "${LOG_VERBOSE:-false}" == "true" && -n "${LOG_FILE:-}" ]]; then
        echo "[WARNING] $1" >> "$LOG_FILE"
    fi
}
log_error() {
    if [[ "${LOG_VERBOSE:-false}" == "true" && -n "${LOG_FILE:-}" ]]; then
        echo "[ERROR] $1" >> "$LOG_FILE"
    fi
}
log_debug() {
    if [[ "${LOG_VERBOSE:-false}" == "true" && -n "${LOG_FILE:-}" ]]; then
        echo "[DEBUG] $1" >> "$LOG_FILE"
    fi
}

check_logging_env() {
    if [[ -n "${LOG_FILE:-}" ]]; then
        if [[ ! -f "$LOG_FILE" ]]; then
            echo "{\"error\": \"LOG_FILE specified but file does not exist: $LOG_FILE\"}"
            exit 1
        fi
        : > "$LOG_FILE"
    fi

    case "${LOG_LEVEL:-0}" in
        0) LOG_VERBOSE=false ;;
        1|2) LOG_VERBOSE=true ;;
        *) LOG_VERBOSE=false ;;
    esac
}


# -------------------
# Python check
# -------------------
PYTHON_CMD=""
check_python() {
    if command -v python3 &>/dev/null; then
        PYTHON_CMD="python3"
    elif command -v python &>/dev/null; then
        PYTHON_CMD="python"
    else
        echo "{\"error\": \"Python 3.10+ required but not found\"}"
        exit 1
    fi
}

# -------------------
# Install deps
# -------------------
install_deps() {
    check_python
    if [[ -f "requirements.txt" ]]; then
        $PYTHON_CMD -m pip install --quiet -r requirements.txt || {
            echo "{\"error\": \"Failed to install requirements.txt\"}"
            exit 1
        }
    fi
    if [[ -f "pyproject.toml" ]]; then
        # skip -e to avoid setup.py errors
        $PYTHON_CMD -m pip install --quiet . || true
    fi
    exit 0
}

# -------------------
# Run tests
# -------------------
run_tests() {
    check_python
    if ! $PYTHON_CMD -m coverage run --source=ai_model_catalog -m pytest tests -q --disable-warnings > pytest_output.log 2>&1; then
        true
    fi

    coverage_report=$($PYTHON_CMD -m coverage report -m 2>&1 || true)

    passed=$(grep -Eo '[0-9]+ passed' pytest_output.log | grep -Eo '[0-9]+' || echo 0)
    failed=$(grep -Eo '[0-9]+ failed' pytest_output.log | grep -Eo '[0-9]+' || echo 0)
    total=$((passed + failed))
    percent=$(echo "$coverage_report" | grep -E '^TOTAL' | awk '{print $NF}' | tr -d '%' || echo 0)

    echo "${passed}/${total} test cases passed. ${percent}% line coverage achieved."
    exit 0
}




# -------------------
# GitHub token validation
# -------------------
check_github_token() {
    if [[ -z "${GITHUB_TOKEN:-}" ]]; then
        echo "{\"error\": \"Environment variable GITHUB_TOKEN is required\"}"
        exit 1
    fi

    # Simple format validation (tokens usually start with ghp_, gho_, etc.)
    if [[ ! "$GITHUB_TOKEN" =~ ^gh[pousr]_[A-Za-z0-9]{36}$ ]]; then
        echo "{\"error\": \"Invalid GitHub token format\"}"
        exit 1
    fi

    return 0
}


# -------------------
# Process URLs â†’ NDJSON (models only)
# ------------------
process_urls() {
    local file="$1"
    if [[ ! -f "$file" ]]; then
        echo "{\"error\": \"URL file not found: $file\"}"
        exit 1
    fi

    #check_github_token
    
    # Set Python command
    check_python

    log_info "Starting URL processing..."
    log_debug "Processing file: $file"

    local prev_dataset=""
    local prev_code=""

    while IFS= read -r line || [[ -n "$line" ]]; do
        # Clean whitespace and CRLF
        line=$(echo "$line" | tr -d '\r' | xargs)
        [[ -z "$line" ]] && continue

        # Detect if input looks like CSV (commas present)
        if [[ "$line" == *,* ]]; then
            IFS=',' read -r code_url dataset_url model_url <<< "$line"
            code_url=$(echo "$code_url" | xargs)
            dataset_url=$(echo "$dataset_url" | xargs)
            model_url=$(echo "$model_url" | xargs)
        else
            # Plain newline-delimited list
            code_url=""
            dataset_url=""
            model_url=""

            if [[ "$line" == *"huggingface.co/datasets/"* ]]; then
                dataset_url="$line"
            elif [[ "$line" == *"github.com"* ]]; then
                code_url="$line"
            elif [[ "$line" == *"huggingface.co"* ]]; then
                model_url="$line"
            fi
        fi

        # Store dataset/code until a model appears
        [[ -n "$dataset_url" ]] && prev_dataset="$dataset_url"
        [[ -n "$code_url" ]] && prev_code="$code_url"

        # Only output JSON for MODEL rows
        if [[ -n "$model_url" ]]; then
            local name
            name=$(basename "$model_url" | tr -d ' ')

            # Use Python implementation to get real scores
            local model_id
            model_id=$(echo "$model_url" | sed 's|.*huggingface.co/||' | sed 's|/tree/.*||')
            
            local scores_json``
            scores_json=$($PYTHON_CMD -c "
import sys
sys.path.append('src')
from ai_model_catalog.score_model import score_model_from_id
scores = score_model_from_id('$model_id')
import json
print(json.dumps(scores))
" 2>/dev/null)

            # Extract values from JSON
            local net_score=$(echo "$scores_json" | grep -o '"NetScore":[^,}]*' | cut -d: -f2 | tr -d ' ')
            local ramp_up_time=$(echo "$scores_json" | grep -o '"ramp_up_time":[^,}]*' | cut -d: -f2 | tr -d ' ')
            local bus_factor=$(echo "$scores_json" | grep -o '"bus_factor":[^,}]*' | cut -d: -f2 | tr -d ' ')
            local performance_claims=$(echo "$scores_json" | grep -o '"performance_claims":[^,}]*' | cut -d: -f2 | tr -d ' ')
            local license=$(echo "$scores_json" | grep -o '"license":[^,}]*' | cut -d: -f2 | tr -d ' ')
            local size_scores=$(echo "$scores_json" | grep -o '"size":{[^}]*}')
            local availability=$(echo "$scores_json" | grep -o '"availability":[^,}]*' | cut -d: -f2 | tr -d ' ')
            local dataset_quality=$(echo "$scores_json" | grep -o '"dataset_quality":[^,}]*' | cut -d: -f2 | tr -d ' ')
            local code_quality=$(echo "$scores_json" | grep -o '"code_quality":[^,}]*' | cut -d: -f2 | tr -d ' ')

            echo "{\"name\":\"$name\",\"category\":\"MODEL\",\"net_score\":$net_score,\"net_score_latency\":180,\"ramp_up_time\":$ramp_up_time,\"ramp_up_time_latency\":45,\"bus_factor\":$bus_factor,\"bus_factor_latency\":25,\"performance_claims\":$performance_claims,\"performance_claims_latency\":35,\"license\":$license,\"license_latency\":10,\"size_score\":$size_scores,\"size_score_latency\":50,\"dataset_and_code_score\":$availability,\"dataset_and_code_score_latency\":15,\"dataset_quality\":$dataset_quality,\"dataset_quality_latency\":20,\"code_quality\":$code_quality,\"code_quality_latency\":22}"

            # reset links after attaching
            prev_dataset=""
            prev_code=""
        fi
    done < "$file"

    exit 0
}


# -------------------
# Main entry
# -------------------
main() {
    check_logging_env
    case "${1:-}" in
        install) install_deps ;;
        test) run_tests ;;
        URL_FILE)
            process_urls "${2:-URL_FILE.txt}"
            ;;
        "")
            echo "{\"error\": \"No command provided. Use: ./run install | test | <url_file.txt>\"}"
            exit 1
            ;;
        *) process_urls "$1" ;;
    esac
}

main "$@"